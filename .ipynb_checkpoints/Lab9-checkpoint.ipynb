{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6efc9db",
   "metadata": {
    "id": "c6efc9db"
   },
   "source": [
    "## Object Following (50 pts)\n",
    "\n",
    "In this notebook we'll show how you can follow an object with the crazyflie!  We'll use a pre-trained neural network that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices). We use the MobileNet SSD (Single Shot Detector) trained on the COCO dataset. SSD models are often faster than other detection models and the MobileNet backbone is less computationally intensive, so this will help for real-time execution! The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\n",
    "which provides utilities for training object detectors for custom tasks also!\n",
    "\n",
    "We won't run through all of the training and optimization steps in this notebook though. The goal here is to demonstrate what one can do with neural networks. In the final project, you will get a chance to train neural networks for obstacle avoidance and navigation. \n",
    "\n",
    "Anyways, let's get started!  First, we will load the pre-trained network. Make sure to have the Lab9_Supplement directory downloaded. Also download the model and place in the Lab9_Supplement directory [https://drive.google.com/file/d/1vIS9XySf5kdmVqPCtCpHG_-FL6RB8oOP/view](https://drive.google.com/file/d/1vIS9XySf5kdmVqPCtCpHG_-FL6RB8oOP/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dac27",
   "metadata": {
    "id": "d84dac27"
   },
   "source": [
    "### Compute detections on single camera image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcaa92b",
   "metadata": {
    "id": "1fcaa92b"
   },
   "source": [
    "For this lab, we will be using OpenCV's DNN module which provides us with functionalities for deep learning inference. You can read more about how we are using it for [object detection](https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/). Specifically, we can load in the MobileNet SSD network that was trained using the Tensorflow framework. OpenCV's DNN module allows for multi-framework use (e.g., PyTorch and Caffe).\n",
    "\n",
    "First, we load in the COCO class names (e.g., person, potted plant, etc.), assign colors to the classes (this is useful for visualizing bounding boxes), and load the weights of the pre-trained neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dac42c76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "error",
     "timestamp": 1638317996802,
     "user": {
      "displayName": "Justin Bi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "05596507647500421977"
     },
     "user_tz": 300
    },
    "id": "dac42c76",
    "outputId": "a4ed2a1a-edd3-4dd1-e6d1-581e82d7b831"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# load the COCO class names\n",
    "with open('Lab9_Supplement/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "    \n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# load the DNN model\n",
    "model = cv2.dnn.readNet(model='Lab9_Supplement/frozen_inference_graph.pb',\n",
    "                        config='Lab9_Supplement/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt', \n",
    "                        framework='TensorFlow')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d2671",
   "metadata": {
    "id": "711d2671"
   },
   "source": [
    "Now we will prepare an image for object detection with our model. `blobFromImage()` prepares the image into the correct format for our model. Specifically, we resize our input image to 300x300 and normalize the RGB channels with the mean parameter. Then we forward propagate the image through the model to obtain the detections. Each detection is of the form ( _, class_id, confidence, box_x, box_y, box_width, box_height) where box_x, box_y, box_width, box_height provide information for creating the bounding box of around the detected object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3fb123",
   "metadata": {
    "id": "5e3fb123"
   },
   "outputs": [],
   "source": [
    "# read the image from disk\n",
    "image = cv2.imread('Lab9_Supplement/lab9_image.jpg')\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "# create blob from image\n",
    "blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), \n",
    "                             swapRB=True)\n",
    "\n",
    "# create blob from image\n",
    "model.setInput(blob)\n",
    "\n",
    "# forward pass through the model to carry out the detection\n",
    "detections = model.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f121a",
   "metadata": {
    "id": "532f121a"
   },
   "source": [
    "Next we visualize the detections. You should see a bounding box, classification, and confidence value appear around each COCO object (potted plant and cup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aace307b",
   "metadata": {
    "id": "aace307b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over each of the detection\n",
    "for detection in detections[0, 0, :, :]:\n",
    "    # extract the confidence of the detection\n",
    "    confidence = detection[2]\n",
    "    # draw bounding boxes only if the detection confidence is above...\n",
    "    # ... a certain threshold, else skip\n",
    "    if confidence > .4:\n",
    "        # get the class id\n",
    "        class_id = detection[1]\n",
    "        # map the class id to the class\n",
    "        class_name = class_names[int(class_id)-1]\n",
    "        color = COLORS[int(class_id)]\n",
    "        # get the bounding box coordinates\n",
    "        box_x = detection[3] * image_width\n",
    "        box_y = detection[4] * image_height\n",
    "        # get the bounding box width and height\n",
    "        box_width = detection[5] * image_width\n",
    "        box_height = detection[6] * image_height\n",
    "        # draw a rectangle around each detected object\n",
    "        cv2.rectangle(image, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=1)\n",
    "        # put the FPS text on top of the frame\n",
    "        text = class_name + ' ' + '%.2f' % (confidence)\n",
    "        cv2.putText(image, text, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, color, 1)\n",
    "\n",
    "while(True):\n",
    "    cv2.imshow('image', image)\n",
    "    \n",
    "    # Hit q to quit.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imwrite('Lab9_Supplement/image_result.jpg', image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598a251",
   "metadata": {
    "id": "4598a251"
   },
   "source": [
    "To print just the first object detected in the example image, we could call the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fe003f",
   "metadata": {
    "id": "96fe003f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         64.          0.87820435  0.38115057  0.31652772  0.618363\n",
      "  0.7122183 ]\n"
     ]
    }
   ],
   "source": [
    "object_number = 0\n",
    "print(detections[0, 0, object_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534903d",
   "metadata": {
    "id": "2534903d"
   },
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified category (e.g., person, etc.).  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision; this is the 'target' object\n",
    "3.  Control the robot towards target object; otherwise hover\n",
    "\n",
    "We'll also create a controller that will use the distance between the target object and the center of the robot's field of view to follow the object as well as use the bounding box size to determine when to stop. \n",
    "\n",
    "First, let's define some functions that will process the images from the crazyflie. \n",
    "\n",
    "### Task 1 (10 pts) ###\n",
    "\n",
    "Fill in the function \"closest_detection\" below. This should find the detected object that is closest to the center of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8261deb4",
   "metadata": {
    "id": "8261deb4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    center_x = (detection[3] + detection[5]) / 2.0 - 0.5\n",
    "    center_y = (detection[4] + detection[6]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "\n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"TODO: Find the detection closest to the image center\"\"\"\n",
    "    # Loop through and find the detection that is closest to the image center\n",
    "    # You can use the detection_center function above to find the center of the detected object\n",
    "    # Note that the origin (i.e., (x,y) = (0,0)) corresponds to the center of the image. So you can\n",
    "    # use the \"norm\" function above to find the detection that is closest to the center.\n",
    "    # Return the det that corresponds to the closest detection to the image center.\n",
    "    # If nothing is detected, return None.\n",
    "    \n",
    "    # Each detection is of the form ( _, class_id, confidence, box_x, box_y, box_width, box_height) \n",
    "    # where box_x, box_y, box_width, box_height provide information for creating the bounding box of around the detected object.\n",
    "    if detections is None:\n",
    "        return None\n",
    "    else:\n",
    "        curr_closest_dist = np.inf\n",
    "        curr_closest_det = None\n",
    "        for d in detections:\n",
    "            center = detection_center(d)\n",
    "            if norm(center) < curr_closest_dist:\n",
    "                curr_closest_dist = norm(center)\n",
    "                curr_closest_det = d\n",
    "        return curr_closest_det\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9914ca",
   "metadata": {
    "id": "6d9914ca"
   },
   "source": [
    "Great, now let's get ready to control the crazyflie to follow an object! Below are a few functions to help move the crazyflie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48aa82bf",
   "metadata": {
    "id": "48aa82bf"
   },
   "outputs": [],
   "source": [
    "import cflib.crtp\n",
    "from cflib.crazyflie import Crazyflie\n",
    "from cflib.crazyflie.log import LogConfig\n",
    "from cflib.crazyflie.syncCrazyflie import SyncCrazyflie\n",
    "from cflib.crazyflie.syncLogger import SyncLogger\n",
    "from cflib.positioning.position_hl_commander import PositionHlCommander\n",
    "from cflib.positioning.motion_commander import MotionCommander\n",
    "\n",
    "\n",
    "def wait_for_position_estimator(scf):\n",
    "    print('Waiting for estimator to find position...')\n",
    "\n",
    "    log_config = LogConfig(name='Kalman Variance', period_in_ms=500)\n",
    "    log_config.add_variable('kalman.varPX', 'float')\n",
    "    log_config.add_variable('kalman.varPY', 'float')\n",
    "    log_config.add_variable('kalman.varPZ', 'float')\n",
    "\n",
    "    var_y_history = [1000] * 10\n",
    "    var_x_history = [1000] * 10\n",
    "    var_z_history = [1000] * 10\n",
    "\n",
    "    threshold = 0.001\n",
    "    with SyncLogger(scf, log_config) as logger:\n",
    "        for log_entry in logger:\n",
    "            data = log_entry[1]\n",
    "\n",
    "            var_x_history.append(data['kalman.varPX'])\n",
    "            var_x_history.pop(0)\n",
    "            var_y_history.append(data['kalman.varPY'])\n",
    "            var_y_history.pop(0)\n",
    "            var_z_history.append(data['kalman.varPZ'])\n",
    "            var_z_history.pop(0)\n",
    "\n",
    "            min_x = min(var_x_history)\n",
    "            max_x = max(var_x_history)\n",
    "            min_y = min(var_y_history)\n",
    "            max_y = max(var_y_history)\n",
    "            min_z = min(var_z_history)\n",
    "            max_z = max(var_z_history)\n",
    "\n",
    "            print(\"{} {} {}\".\n",
    "                format(max_x - min_x, max_y - min_y, max_z - min_z))\n",
    "\n",
    "            if (max_x - min_x) < threshold and (\n",
    "                    max_y - min_y) < threshold and (\n",
    "                    max_z - min_z) < threshold:\n",
    "                break\n",
    "\n",
    "def set_PID_controller(cf):\n",
    "    # Set the PID Controller:\n",
    "    print('Initializing PID Controller')\n",
    "    cf.param.set_value('stabilizer.controller', '1')\n",
    "    cf.param.set_value('kalman.resetEstimation', '1')\n",
    "    time.sleep(0.1)\n",
    "    cf.param.set_value('kalman.resetEstimation', '0')\n",
    "    \n",
    "    wait_for_position_estimator(cf)\n",
    "    time.sleep(0.1)    \n",
    "    return\n",
    "\n",
    "# Ascend and hover:\n",
    "def ascend_and_hover(cf):\n",
    "    # Ascend:\n",
    "    for y in range(5):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, y / 10)\n",
    "        time.sleep(0.1)\n",
    "    # Hover at 0.5 meters:\n",
    "    for _ in range(20):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n",
    "        time.sleep(0.1)\n",
    "    return\n",
    "\n",
    "def hover(cf):\n",
    "    print('Hovering:')\n",
    "    # Hover at 0.5 meters\n",
    "    for _ in range(30):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n",
    "        time.sleep(0.1)\n",
    "    return\n",
    "    \n",
    "# Hover, descend, and stop all motion:\n",
    "def hover_and_descend(cf):\n",
    "    # Hover at 0.5 meters:\n",
    "    for _ in range(30):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, 0.5)\n",
    "        time.sleep(0.1)\n",
    "    # Descend:\n",
    "    for y in range(10):\n",
    "        cf.commander.send_hover_setpoint(0, 0, 0, (10 - y) / 25)\n",
    "        time.sleep(0.1)\n",
    "    # Stop all motion:\n",
    "    for i in range(10):\n",
    "        cf.commander.send_stop_setpoint()\n",
    "        time.sleep(0.1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7bbad",
   "metadata": {
    "id": "91d7bbad"
   },
   "source": [
    "### Task 2 (20 pts) ###\n",
    "\n",
    "Fill in the controller below that says \"TODO\" to make the crazyflie follow the object. The controller should use the inputs to keep the detected target in the center of its view as well determine when to stop (send True flag) so that the crazyflie stops and lands before crashing into the tracked object. (Note: the execution code implements the actual stopping using the flag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba85d2d4",
   "metadata": {
    "id": "ba85d2d4"
   },
   "outputs": [],
   "source": [
    "def controller(cf, box_x, box_y, box_width, box_height, x_cur, y_cur):\n",
    "    \"\"\"\n",
    "    \n",
    "    cf: crazyflie instance\n",
    "    box_x: x coordinate of the center of the bounding box in the image\n",
    "    box_y: y coordinate of the center of the bounding box in the image\n",
    "    box_width: width of the bounding box in the image\n",
    "    box_height: height of the bounding box in the image\n",
    "    x_cur: current x position\n",
    "    y_cur: current y position\n",
    "    \n",
    "    Return True to indicate that the drone is close to the target and thus exit the loop to stop and descend, new x, new y\n",
    "    Return False to indicate continuing to follow the target, new x, new y.\n",
    "    \n",
    "    \"\"\"\n",
    "    # assume that the drone does not turn\n",
    "    # if bounding box is on the left, then increase y (y is to the left, x is move forward)\n",
    "    # box_x and box_y should be in units of pixels (300 by 300)\n",
    "    # x_cur and y_cur are in units of meters \n",
    "    \n",
    "    # scaling factor makes changes in x and y that are proportional to location of bounding box\n",
    "    scaling_factor = 0.5\n",
    "    \n",
    "    #### TO DO: Fill below ####\n",
    "    # Exit condition/method using size of the bounding box\n",
    "    \n",
    "    # if bounding box is full 300 pixels, then stop\n",
    "    if box_width > .9 or box_height > .9:\n",
    "        return True, x_cur, y_cur\n",
    "    \n",
    "    #### TO DO: Fill below ####\n",
    "    # Determine the x and y to use in send_position_setpoint() \n",
    "    \n",
    "    x_command = x_cur + 0.03\n",
    "    y_command = y_cur - scaling_factor * box_x\n",
    "    \n",
    "    # Set position\n",
    "    cf.commander.send_position_setpoint(x_command, y_command, 0.5, 0)\n",
    "    \n",
    "    return False, x_command, y_command\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d02ec",
   "metadata": {
    "id": "c93d02ec"
   },
   "source": [
    "The following code will test your controller on the crazyflie. There are several parameters at the top that may be useful to change as indicated, otherwise do not modify the code. Please read the safety and submission instructions below before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8813fcee",
   "metadata": {
    "id": "8813fcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning interfaces for Crazyflies...\n",
      "Crazyflies found:\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "radio://0/15/2M\n",
      "radio://0/16/2M\n",
      "radio://0/80/2M\n",
      "Initializing PID Controller\n",
      "Waiting for estimator to find position...\n",
      "999.9999890147437 999.9999890293484 999.9997760536789\n",
      "999.9999894003449 999.9999893994673 999.999777331177\n",
      "999.9999894463908 999.9999894630228 999.999777331177\n",
      "999.9999894463908 999.9999894630228 999.999777331177\n",
      "999.9999894463908 999.9999894630228 999.9997792980139\n",
      "999.9999894673847 999.9999894630228 999.9997813608788\n",
      "999.9999894673847 999.9999894630228 999.9997813608788\n",
      "999.9999894673847 999.9999894630228 999.9997813608788\n",
      "999.9999894673847 999.9999894630228 999.9997813608788\n",
      "1.1036609066650271e-06 1.1022184480680153e-06 2.314230368938297e-05\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "no detection...hovering\n",
      "Hovering:\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n",
      "detection...tracking\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# load the COCO class names\n",
    "with open('Lab9_Supplement/object_detection_classes_coco.txt', 'r') as f:\n",
    "    class_names = f.read().split('\\n')\n",
    "\n",
    "# get a different color array for each of the classes\n",
    "COLORS = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# load the DNN model\n",
    "model = cv2.dnn.readNet(model='Lab9_Supplement/frozen_inference_graph.pb',\n",
    "                        config='Lab9_Supplement/ssd_mobilenet_v2_coco_2018_03_29.pbtxt.txt', \n",
    "                        framework='TensorFlow')\n",
    "\n",
    "# ************ Parameters that might be useful to change ************ \n",
    "# COCO label id that we want to track\n",
    "tracking_label = 62\n",
    "\n",
    "# Set the URI the Crazyflie will connect to\n",
    "group_number = 16\n",
    "uri = f'radio://0/{group_number}/2M'\n",
    "\n",
    "# Possibly try 0, 1, 2 ...\n",
    "camera = 2\n",
    "\n",
    "# Confidence of detection\n",
    "confidence = 0.4\n",
    "\n",
    "# ******************************************************************\n",
    "\n",
    "# Initialize all the CrazyFlie drivers:\n",
    "cflib.crtp.init_drivers(enable_debug_driver=False)\n",
    "\n",
    "# Scan for Crazyflies in range of the antenna:\n",
    "print('Scanning interfaces for Crazyflies...')\n",
    "available = cflib.crtp.scan_interfaces()\n",
    "\n",
    "# List local CrazyFlie devices:\n",
    "print('Crazyflies found:')\n",
    "for i in available:\n",
    "    print(i[0])\n",
    "\n",
    "if len(available) == 0:\n",
    "    print('No Crazyflies found, cannot run example')\n",
    "else:\n",
    "    ## Ascend to hover; run the sequence; then descend from hover:\n",
    "    # Use the CrazyFlie corresponding to team number:\n",
    "    with SyncCrazyflie(uri, cf=Crazyflie(rw_cache='./cache')) as scf:\n",
    "        # Get the Crazyflie class instance:\n",
    "        cf = scf.cf\n",
    "\n",
    "        # Initialize and ascend:\n",
    "        t = time.time()\n",
    "        elapsed = time.time() - t\n",
    "        ascended_bool = 0\n",
    "\n",
    "        # capture the video\n",
    "        cap = cv2.VideoCapture(camera)\n",
    "        \n",
    "        # get the video frames' width and height\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "\n",
    "        # flag indicating whether to exit the main loop and then descend\n",
    "        exit_loop = False\n",
    "\n",
    "        # Ascend and hover a bit\n",
    "        set_PID_controller(cf)\n",
    "        ascend_and_hover(cf)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        x_cur = 0\n",
    "        y_cur = 0\n",
    "        \n",
    "        # detect objects in each frame of the video\n",
    "        while cap.isOpened() and not exit_loop:\n",
    "            \n",
    "            # Try to read image\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                image = frame\n",
    "                image_height, image_width, _ = image.shape\n",
    "\n",
    "                # create blob from image\n",
    "                blob = cv2.dnn.blobFromImage(image=image, size=(300, 300), mean=(104, 117, 123), \n",
    "                                             swapRB=True)\n",
    "\n",
    "                # forward propagate image\n",
    "                model.setInput(blob)\n",
    "                detections = model.forward()\n",
    "\n",
    "                # select detections that match selected class label\n",
    "                matching_detections = [d for d in detections[0, 0] if d[1] == tracking_label]\n",
    "\n",
    "                # select confident detections\n",
    "                confident_detections = [d for d in matching_detections if d[2] > confidence]\n",
    "\n",
    "                # get detection closest to center of field of view and draw it\n",
    "                det = closest_detection(confident_detections) # This relies on the function you wrote above\n",
    "                \n",
    "                if det is not None:\n",
    "                    # get the class id\n",
    "                    class_id = det[1]\n",
    "                    # map the class id to the class \n",
    "                    class_name = class_names[int(class_id)-1]\n",
    "                    color = COLORS[int(class_id)]\n",
    "                    # get the bounding box coordinates\n",
    "                    box_x = det[3] * image_width\n",
    "                    box_y = det[4] * image_height\n",
    "                    # get the bounding box width and height\n",
    "                    box_width = det[5] * image_width\n",
    "                    box_height = det[6] * image_height\n",
    "                    # draw a rectangle around each detected object\n",
    "                    cv2.rectangle(image, (int(box_x), int(box_y)), (int(box_width), int(box_height)), color, thickness=2)\n",
    "                    # put the class name text on the detected object\n",
    "                    cv2.putText(image, class_name, (int(box_x), int(box_y - 5)), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "                # If nothing is detected, hover\n",
    "                if det is None:\n",
    "                    print('no detection...hovering')\n",
    "                    hover(cf)\n",
    "\n",
    "                # otherwise  move towards target\n",
    "                else:\n",
    "                    print('detection...tracking')\n",
    "                    _, _, _, box_x, box_y, box_width, box_height = det\n",
    "                    box_x, box_y = detection_center(det)\n",
    "                    exit_loop, x_cur, y_cur = controller(cf, box_x, box_y, box_width, box_height, x_cur, y_cur)\n",
    "\n",
    "                # Check image\n",
    "                cv2.imshow('image', image)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "            else:\n",
    "                print('no image!!')\n",
    "                \n",
    "        cap.release()\n",
    "        \n",
    "        # Descend and stop all motion:\n",
    "        hover_and_descend(cf)\n",
    "        \n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f80957",
   "metadata": {
    "id": "38f80957"
   },
   "source": [
    "# Submission #\n",
    "\n",
    "Please submit to Gradescope \"HW9: Coding\" a zip including: this notebook Lab9 (30pts), two videos (20pts see below), and Lab10 notebook (50pts).\n",
    "\n",
    "For videos, please submit the following:\n",
    "- (10 pts) A video (e.g., taken from your cellphone) showing the crazyflie following you (or any other person). The person should be moving such that it is clear the crazyflie is changing its tracking to follow the person. Read safety instructions below before trying! The crazyflie should stop and land when close to the person.\n",
    "- (10 pts) A video showing the crazyflie moving towards a different object (i.e., not a person). For this, you will have to change the \"tracking label\" in the code above to correspond to the object you want the crazyflie to follow/move towards. You are welcome to choose any object that is convenient for you. For example, you can place a chair (or whatever object you choose) in front of the crazyflie and demonstrate that your code makes the crazyflie move towards that object. The crazyflie should stop and land when close to the object without running into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0f7ec",
   "metadata": {
    "id": "14d0f7ec"
   },
   "source": [
    "# Safety #\n",
    "\n",
    "As always, please wear your safety glasses when working with the crazyflie. \n",
    "\n",
    "Additionally, for human tracking, please stand OUTSIDE of the netted test space. The drone's camera is capable of detecting people standing behind the net. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
